from langchain_groq import ChatGroq
from dotenv import load_dotenv

load_dotenv()

chat_model = ChatGroq(
    model="llama-3.1-8b-instant",
    temperature=0
)

response = chat_model.invoke("Could you explain what an LLM is?")

print(response.content)
